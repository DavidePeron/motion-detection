% !TEX root = template.tex

\section{Summary of the project}
\label{sec:summary}
This work has been useful to approach new machine learning tools both from a theoretical and a practical point of view, understanding the problems that one may encounter.
In specific, we saw how \gls{ann} and \gls{cnn} work and how can be implemented to solve a given task, we learned to use deep learning framework like Keras and TensorFlow together with advanced version control software. Moreover we improved our knowledge about Python and how to optimize it to work with big amount of data.

During the project we encountered several problems, we tried to solve them and we found a solution or a reasonable answer.
Firstly, we noticed that the number of sample for each label is highly unbalanced, the solution we thought were to use data augmentation and to weight different each label in the \gls{cnn}. We tried to implement the first solution creating new samples adding noise to the original signals. We tried with white noise with small variance but the result was a low accuracy. Maybe the solution is to estimate a good model for the noise and adding it instead of a standard white noise.
In the second case we tried to weight different the classes but we can't adapt our architecture to the requirements of the apposite keras' function.
Our solution has been to use a smaller stride for these activities to have more data.

In the learning phase the biggest problem was the time needed to train the network. We solved this problem installing CUDA drivers in Windows 10 and using the Graphic Card to learn the model.

In the last part, the main problem was how to elaborate the windows of the benchmark signals containing the transitions from an activity to another. Our first idea was to add to the training set some window correspondent to the transitions, and assign to them the label \textit{TRANSIT} (i.e. \textit{activity not recognized}), but the only result was a lower accuracy, so we decided to skip those parts when sliding other the benchmark signals.

Finally, we notice that in the benchmark dataset misses the label \textit{TRANSDCC}, so both precision and recall has been put to zero even if these label were not present at all in the prediction phase.

\section{Conclusions and future works}
\label{sec:conclusions}

In this work, a novel approach to activity recognition has been proposed, using \gls{cnn} as deep learning tool to predict automatically activities.
The authors started from a dataset used originally to make activity recognition using a feature selection approach, they applied some preprocessing and they used the preprocessed data to learn the Neural Network.
Then, the learned model has been applied to raw signals to show how this architecture is feasible for a real time system.
Remarkable results has been obtained, an accuracy of over $94\%$ in testing phase and an overall recognition rate of $75\%$ in the real time prediction.

\red{This section should take max half a page.}

\MR{In many papers, here you find a summary of what done. It is basically an abstract where instead of using the present tense you use the past participle, as you refer to something that you have already developed in the previous sections. While I did it myself in the past, I now find it rather useless.}\\

\MR{\textbf{What I would like to see here is:} 1) a very short summary of what done, 2) some (possibly) intelligent observations on the relevance and {\it applicability} of your algorithms / findings, 3) what is still missing, and can be done in the future to extend your work. The idea is that this section should be {\it useful} and not just a repetition of the abstract (just \mbox{re-phrased} and written using a different tense...).}\\

\MR{\textbf{Moreover:} being a project report, I would also like to see a specific paragraph specifying: 1) what you have learned, and 2) any difficulties you may have encountered.}
